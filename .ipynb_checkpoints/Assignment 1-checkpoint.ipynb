{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYDE 522 Assignment 1\n",
    "## Perceptrons and Regression\n",
    "\n",
    "### Due: Monday Sept 25 at 11:59pm\n",
    "\n",
    "As with all the assignments in this course, this assignment is structured as a Jupyter Notebook and uses Python.  If you do not have Python and Jupyter Notebook installed, the easiest method is to download and install Anaconda https://www.anaconda.com/download.  There is a quick tutorial for running Jupyter Notebook from within Anacoda at https://docs.anaconda.com/free/anaconda/getting-started/hello-world/#python-exercise-jupyter under \"Run Python in a Jupyter Notebook\"\n",
    "\n",
    "Implement your assignment directly in the Jupyter notebook and submit your resulting Jupyter Notebook file using Learn.\n",
    "\n",
    "While you are encouraged to talk about the assignment with your classmates, you must write and submit your own assignment.  Directly copying someone else's assignment and changing a few small things here and there does not count as writing your own assignment.\n",
    "\n",
    "Make sure to label the axes on all of your graphs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Implementing a Perceptron\n",
    "\n",
    "The following code generates the same data that was used to demonstrate the Perceptron in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "data_x, data_y = sklearn.datasets.make_blobs(centers=[[-2, -2], [2, 2]], \n",
    "                                             cluster_std=[0.3, 1.5], \n",
    "                                             random_state=0, \n",
    "                                             n_samples=200, \n",
    "                                             n_features=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces two arrays, `data_x` which contains the input data (200 rows, each of which has 2 values $x_1$ and $x_2$), and `data_y` which contains the desired output data (either a 1 or a 0).\n",
    "\n",
    "Implement a Perceptron to learn a classifier on this data.  It should learn three values: $\\omega_1$, $\\omega_2$, and $\\theta$ (of course you can use whatever variable names you like to encode them).  You can treat $\\theta$ separately, or you can consider it an extra weight variable $\\omega_0$ and have an extra input that is always 1.  Implement this Perceptron yourself, rather than using the `sklearn.linear_model.Perceptron` implementation that we will use in Question 2.\n",
    "\n",
    "Initialize the weights to $\\omega_1=1; \\omega_2=-1; \\theta=0$.  \n",
    "\n",
    "**a) [1 mark]** Before doing any training, plot the data as a scatterplot and colour the dots such that the data points for which the model outputs a 1 are blue and the ones for which the model outputs a 0 are red.  This can be done with the following code, if `y` is the list of outputs from your model.  Compute how accurate the model is (i.e. what percentage of the time the model outputs the correct value) and report that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(data_x[:,\u001b[38;5;241m0\u001b[39m], data_x[:,\u001b[38;5;241m1\u001b[39m], c\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mwhere(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$x_1$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$x_2$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(data_x[:,0], data_x[:,1], c=np.where(y, 'blue', 'red'))\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) [1 mark]** Train the model by going through each of the 200 elements in the data set in order once.  For each input, check if the output is correct.  If it is not correct, apply the Perceptron Learning Rule.   Use a learning rate of 0.1.\n",
    "\n",
    "Now produce the same plot as in part a), but with your trained weights.  How accurate is the model now?  Report the $\\omega$ and $\\theta$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) [1 mark]** Repeat the training in part b) enough times that the model is perfect (in that it correctly classifies all the inputs).  How many repetitions does this take?  Produce the same plots as in part a) and b), but with your new weights.  Report the $\\omega$ and $\\theta$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) [1 mark]** Create a new Perceptron identical to the above one, but with a learning rate of 1.0.  Train this model until it reaches 100% accuracy.  How many repetitions does this take?  Produce the same plot again, but with your new weights.  Report the $\\omega$ and $\\theta$ values.\n",
    "\n",
    "Now do the same thing with a learning rate of 0.01, and then again with a learning rate of 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "\n",
    "We will now try a more complex dataset, and use a pre-written implementation of the Perceptron.  The `sklearn` Python library https://scikit-learn.org/ has a large collection of machine learning algorithms, and comes with a variety of datasets.  It comes pre-installed with Anaconda or can be installed with `pip install scikit-learn`.\n",
    "\n",
    "The dataset we will use is the UCI ML hand-written digits dataset https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
    "\n",
    "It is available with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "digits = sklearn.datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs are 64 values, representing an 8x8 input image that is a low-resolution handwritten digit.  You can access this data as `digits.data`.  The correct label (i.e. the desired output) for each digit is accessed with `digits.target`.  Here are the first four input-output pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArkAAADACAYAAAAA7HuUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUDElEQVR4nO3df2zcd33H8ddrSVPa0sbuam2QFBxLLFIntW7r8WOdWFocCWiFvYlGwGC4TEunqawZTDhM+yOVYEv+YS4aYwktFI0O1hSUdKijtGvNBmMFu3U3tSFT8wM1WTtc6pTARgPte3/clbnB8V3iz9338/3k+ZBOsc/n9719eeXy8vnrO0eEAAAAgJL8QtULAAAAAKlRcgEAAFAcSi4AAACKQ8kFAABAcSi5AAAAKA4lFwAAAMWh5AIAAKA4lNxF2D5oezjX67Y9aHva9v80/xzs0nrIWA1yu8P2Xtsv2B7r0mqogZyza/tXbO+2PWv7Gdv32F7bzR2Rr8yze4Htb9j+vu0jtr9p+4pu7lgVSm6H2F7W4fkrJO2W9DlJvZI+K2l383zglHQ6t02PSPpDSQ914bpwmuhCdnsk3SVpraRfkvQtNe6DgSXpQnZ/KOl9kvrU6AvbJP2D7eUdvt7KUXJPwPbfSnqVGkH4oe0P2d5p+ynbz9r+Z9u/Ou/yt9n+pO27bf9I0pW2L7P9sO2jzc/9e9sfmfc519ieaX5n9a+2Lz7RdS+w4jpJyyVNRMRzEfFxSZZ0VcduFGSvBrlVRHwiIv5J0o87eVugXnLPbkR8KyJujYhnIuInkv5S0lrbv9jhmwaZq0F2fxwReyPiBTV6wvNqlN3zO3rD5CAiOJ3gJOmgpOF5779P0rmSzpQ0IWlm3sduk/SspCvU+ObhPEnflXSjpDMk/bakY5I+0rz8pZK+J+l1kpZJem/z+s5c6Lqb5/27pHc13/5jSf943Me/LOmDVd9unKo95Zzb487/uqSxqm8vTvmc6pLd5sdGJT1Z9W3GKY9THbLbPO+YpJD0qapvs26ceCT3JETEpyPiaEQ8J2mLpEtsr5x3kd0R8Y1ofLc0qMYjrR+PiJ9ExJfU+PHWizZK2h4RD0bE8xHxWUnPSXr9Itd/cUT8XfPdl6vxj2S+Z9X4RwX8TGa5BdqWa3Ztr5b0CUkfWOKXiELlmN2IuFiNQv0uNR5kKB4lt022l9neanuf7R+o8Z2TJF0w72JPzHv7lZIOR/PbpwU+/mpJH2z+6OGI7SOSLmx+Xjt+qEZY5ztP0tE2Px+ngQxzC7Ql1+za7pP0VUl/HRGfP5nPxekh1+xKPzt04fOSNtu+5GQ/v24ouYubH7h3SRqRNCxppaT+5vk+weWflLTK9vyPXzjv7SckfTQieuadzp53pzl/1kIelXTxcfMvbp6P01vOuQUWk3V2bfeqUXDvioiPtvMF4bSRdXYXcIakgVP4vFqh5C7uv/X/IThXjR8PfF/S2ZL+vMXnflONg7tvsL3c9oik1877+Kck/YHt17nhHNtX237xcIP5172Qyeb8P7J9pu0bmuff3+bXhnLlnFvZXmH7ZWrc4Z9h+2W2uS+ClHF2bZ8n6R5J34iIzSf7haF4OWf39bZ/o3nfe5btcTWeIeTBk/0i64b/WBb3F5L+rPmjgfPVODD8sKTHJP3bYp8YEcfUOHj89yQdkfRuNX4x7Lnmx6ck/b6kv5I0J+lxSWMLXbftP5Ek24/a/p1580cl/W5z/vskjTbPx+kt29w2fVXS/0r6dUk7mm+/8VS/WBQl5+z+lqRfk3Rd87fYXzy9aolfM8qQc3bPVOMY8u83d3qrpKsj4r+W8gXXgV96CAg6yfaDkv4mIj5T9S5Au8gt6orsoq7Ibho8kttBtn/T9i83f/zwXjWOmf1K1XsBiyG3qCuyi7oiu51R/KtdVGytpDsknSNpv6S3R8ST1a4EtERuUVdkF3VFdjuAwxUAAABQHA5XAAAAQHE6crjCBRdcEP39/Z0YfUrm5uaSzTp06FCSOeedd/zrOJya1atXJ5mzbNmyJHNSOXjwoJ5++mm3vmQ6ueU2pb179yaZ8/zzzyeZ88pXpnntiJ6eniRzUpqenn46Ivq6eZ0lZ/fo0TSvb7Nv374kc84666wkc9auXZtkTipV3OdK+WX3qaeeSjbr8OHDSeasWLEiyZyLLrooyZw69YWOlNz+/n5NTU11YvQp2blzZ7JZ4+PjSeasX78+yZytW7cmmdPb25tkTipDQ0Ndv87ccpvSunXrksw5cuRIkjk33XRTkjkjIyNJ5qRk+7vdvs6Sszs5OZlkzujoaJI5g4ODSeak+rpSqeI+V8ovu9u2bUs2a/PmNE+nvGrVqiRz7r8/zdPo16kvcLgCAAAAikPJBQAAQHEouQAAACgOJRcAAADFaavk2n6z7b22H7ed5khqoAvILuqI3KKuyC5y0rLk2l4m6ROS3iLpIknvtJ3meSiADiK7qCNyi7oiu8hNO4/kvlbS4xGxPyKOSfqCpPyetwf4eWQXdURuUVdkF1lpp+SukvTEvPcPNc97CdsbbU/ZnpqdnU21H7AULbNLbpEh7nNRV2QXWUn2i2cRsSMihiJiqK+vqy/2A5wycou6IruoK7KLbmmn5B6WdOG891c3zwNyR3ZRR+QWdUV2kZV2Su63Jb3G9hrbKyS9Q9JdnV0LSILsoo7ILeqK7CIry1tdICJ+avsGSfdIWibp0xHxaMc3A5aI7KKOyC3qiuwiNy1LriRFxN2S7u7wLkByZBd1RG5RV2QXOeEVzwAAAFAcSi4AAACKQ8kFAABAcdo6JrfuxsfHk806cOBAkjlzc3NJ5px//vlJ5txxxx1J5kjStddem2wW0ujp6Uky52tf+1qSOQ888ECSOSMjvJhSjmZmZpLNuvLKK5PMWblyZZI5Bw8eTDIHaW3evDnJnJT/F27fvj3JnOuvvz7JnOnp6SRzhoeHk8zpBh7JBQAAQHEouQAAACgOJRcAAADFoeQCAACgOJRcAAAAFIeSCwAAgOJQcgEAAFAcSi4AAACKQ8kFAABAcSi5AAAAKA4lFwAAAMWh5AIAAKA4lFwAAAAUh5ILAACA4lByAQAAUBxKLgAAAIpDyQUAAEBxlle9wGKmp6eTzDlw4ECSOZK0b9++JHMGBgaSzFm/fn2SOalua0m69tprk806nc3MzCSbNTk5mWxWCoODg1WvgA7atWtXslmXXHJJkjmjo6NJ5tx0001J5iCtjRs3JpkzPj6eZI4kXX755UnmrFmzJsmc4eHhJHPqhEdyAQAAUBxKLgAAAIpDyQUAAEBxKLkAAAAoDiUXAAAAxaHkAgAAoDgtS67tC20/YPsx24/avrEbiwFLRXZRR+QWdUV2kZt2nif3p5I+GBEP2T5X0rTteyPisQ7vBiwV2UUdkVvUFdlFVlo+khsRT0bEQ823j0raI2lVpxcDlorsoo7ILeqK7CI3J3VMru1+SZdKenCBj220PWV7anZ2NtF6QBonyi65Rc64z0VdkV3koO2Sa/vlkr4oaVNE/OD4j0fEjogYioihvr6+lDsCS7JYdsktcsV9LuqK7CIXbZVc22eoEdjbI+JLnV0JSIfsoo7ILeqK7CIn7Ty7giXdKmlPRHys8ysBaZBd1BG5RV2RXeSmnUdyr5D0HklX2Z5pnt7a4b2AFMgu6ojcoq7ILrLS8inEIuLrktyFXYCkyC7qiNyirsgucsMrngEAAKA4lFwAAAAUh5ILAACA4rTzsr6VmZubSzLnsssuSzJHkgYGBpLNSuHyyy+vegUcZ2JiIsmcLVu2JJkjSc8++2yyWSmsW7eu6hXQQZs2bUo2q7+/P8mcVDuNjIwkmYO0Uv3fvH///iRzJOnAgQNJ5gwPDyeZk6pT9fb2JpnTDTySCwAAgOJQcgEAAFAcSi4AAACKQ8kFAABAcSi5AAAAKA4lFwAAAMWh5AIAAKA4lFwAAAAUh5ILAACA4lByAQAAUBxKLgAAAIpDyQUAAEBxKLkAAAAoDiUXAAAAxaHkAgAAoDiUXAAAABSHkgsAAIDiLK96gcXMzc0lmbN+/fokc3KU6jbq7e1NMgfSpk2bkswZGxtLMkfK7+/3yJEjVa+ABaT6e5mYmEgyR5J27dqVbFYKt912W9UroIMGBgaSzXrmmWeSzBkeHs5qzn333ZdkjtT5/5t4JBcAAADFoeQCAACgOJRcAAAAFIeSCwAAgOJQcgEAAFCctkuu7WW2H7b95U4uBKREblFXZBd1RXaRi5N5JPdGSXs6tQjQIeQWdUV2UVdkF1loq+TaXi3pakm3dHYdIB1yi7oiu6grsouctPtI7oSkD0l64UQXsL3R9pTtqdnZ2RS7AUs1IXKLepoQ2UU9TYjsIhMtS67tayR9LyKmF7tcROyIiKGIGOrr60u2IHAqyC3qiuyirsguctPOI7lXSHqb7YOSviDpKtuf6+hWwNKRW9QV2UVdkV1kpWXJjYgPR8TqiOiX9A5J90fEuzu+GbAE5BZ1RXZRV2QXueF5cgEAAFCc5Sdz4YiYlDTZkU2ADiG3qCuyi7oiu8gBj+QCAACgOJRcAAAAFIeSCwAAgOKc1DG53dbb25tkzvT0ok/ZV4m5ubkkc6amppLM2bBhQ5I5QDtmZmaSzBkcHEwyBw1btmxJMufmm29OMielXbt2JZnT09OTZA7Kl6rD3HfffUnmXH/99UnmbNu2LckcSdq6dWuyWQvhkVwAAAAUh5ILAACA4lByAQAAUBxKLgAAAIpDyQUAAEBxKLkAAAAoDiUXAAAAxaHkAgAAoDiUXAAAABSHkgsAAIDiUHIBAABQHEouAAAAikPJBQAAQHEouQAAACgOJRcAAADFoeQCAACgOJRcAAAAFGd51QssZmBgIMmcqampJHMkaefOnVnNSWV8fLzqFQBUbGxsLMmcycnJJHMk6ZFHHkkyZ3R0NMmckZGRJHOuu+66JHOkdDtB2rx5c7JZw8PDSebMzc0lmXPvvfcmmbNhw4Ykc7qBR3IBAABQHEouAAAAikPJBQAAQHEouQAAACgOJRcAAADFaavk2u6xfaft79jeY/sNnV4MSIHsoo7ILeqK7CIn7T6F2M2SvhIRb7e9QtLZHdwJSInsoo7ILeqK7CIbLUuu7ZWS3ihpTJIi4pikY51dC1g6sos6IreoK7KL3LRzuMIaSbOSPmP7Ydu32D6nw3sBKZBd1BG5RV2RXWSlnZK7XNJlkj4ZEZdK+pGkn3tJENsbbU/ZnpqdnU28JnBKWmaX3CJD3OeirsgustJOyT0k6VBEPNh8/041QvwSEbEjIoYiYqivry/ljsCpapldcosMcZ+LuiK7yErLkhsRT0l6wvba5llvkvRYR7cCEiC7qCNyi7oiu8hNu8+u8H5Jtzd/U3K/pOs6txKQFNlFHZFb1BXZRTbaKrkRMSNpqLOrAOmRXdQRuUVdkV3khFc8AwAAQHEouQAAACgOJRcAAADFoeQCAACgOO0+u0IlBgYGkszZtm1bkjmSND4+nmTO0FCa4/Knp6eTzEF+enp6ks0aGRlJMmf37t1J5kxOTiaZMzY2lmQOGgYHB5PMmZmZSTIn5awtW7YkmZPq30B/f3+SOVK6f9+Qent7k83auHFjslkpbNiwIcmc7du3J5nTDTySCwAAgOJQcgEAAFAcSi4AAACKQ8kFAABAcSi5AAAAKA4lFwAAAMWh5AIAAKA4lFwAAAAUh5ILAACA4lByAQAAUBxKLgAAAIpDyQUAAEBxKLkAAAAoDiUXAAAAxaHkAgAAoDiUXAAAABSHkgsAAIDiOCLSD7VnJX23xcUukPR08is/dezTWjd3enVE9HXpuiTVNrdSfjvlto9EdqX8/l5y20fKb6eicyuR3URy20fKJLsdKbntsD0VEUOVXPkC2Ke1HHfqthxvg9x2ym0fKc+dui232yC3faT8dsptn6rkdjuwT2u57MThCgAAACgOJRcAAADFqbLk7qjwuhfCPq3luFO35Xgb5LZTbvtIee7UbbndBrntI+W3U277VCW324F9Wstip8qOyQUAAAA6hcMVAAAAUBxKLgAAAIrT9ZJr+82299p+3Pbmbl//AvtcaPsB24/ZftT2jVXvJEm2l9l+2PaXM9ilx/adtr9je4/tN1S9UxVyyi65bQ/ZbSC7rZHd/OSU2+Y+ZLf1LlnltqvH5NpeJuk/Ja2XdEjStyW9MyIe69oSP7/TKyS9IiIesn2upGlJo1Xu1NzrA5KGJJ0XEddUvMtnJf1LRNxie4WksyPiSJU7dVtu2SW3be9Ddsluu3uR3YzkltvmTmS39S5Z5bbbj+S+VtLjEbE/Io5J+oKkkS7v8BIR8WREPNR8+6ikPZJWVbmT7dWSrpZ0S5V7NHdZKemNkm6VpIg4djrd0c6TVXbJbWtk92fIbgtkN0tZ5VYiu23skl1uu11yV0l6Yt77h1RxQOaz3S/pUkkPVrzKhKQPSXqh4j0kaY2kWUmfaf445Bbb51S9VAWyzS65PSGy20B2W5sQ2c1NtrmVyO4JZJdbfvGsyfbLJX1R0qaI+EGFe1wj6XsRMV3VDsdZLukySZ+MiEsl/UhS5cdGoYHcLorsZozsLorsZozsnlB2ue12yT0s6cJ5769unlcp22eoEdjbI+JLFa9zhaS32T6oxo9nrrL9uQr3OSTpUES8+N3qnWqE+HSTXXbJbUtkt4HsLo7s5im73Epkt4XsctvtkvttSa+xvaZ5QPI7JN3V5R1ewrbVOH5kT0R8rMpdJCkiPhwRqyOiX43b5/6IeHeF+zwl6Qnba5tnvUlSpQfZVySr7JLbtnYiuw1kdxFkN1tZ5VYiu23sk11ul3fzyiLip7ZvkHSPpGWSPh0Rj3ZzhwVcIek9kv7D9kzzvD+NiLurWyk775d0e/OOZr+k6yrep+syzC65bQ/ZJbt1dVpnN8PcSmS3HVnllpf1BQAAQHH4xTMAAAAUh5ILAACA4lByAQAAUBxKLgAAAIpDyQUAAEBxKLkAAAAoDiUXAAAAxfk/ZXo+ujylMZwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(digits.data[i].reshape(8,8), cmap='gray_r')\n",
    "    plt.title(f'target:{digits.target[i]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a Perceptron using the following command, where `eta0` is the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "perceptron = sklearn.linear_model.Perceptron(eta0=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an input data `X` and target output data `Y`, you can train the perceptron as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Perceptron has been trained, you can see what outputs it actually generates given input `X` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, `sklearn` provides a useful tool for separating your data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(\n",
    "    digits.data, digits.target, test_size=0.2, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This splits all your data in `digits.data` into two parts, `X_train` and `X_test` (with the corresponding outputs in `Y_train` and `Y_test`).  The setting `test_size=0.2` means that the test set will be 20% of the data, and `shuffle=True` means it will randomly choose that 20%.\n",
    "\n",
    "Note that you can also use the same function to split your training data into training data and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) [1 mark]** Let's start with only considering the digit data for 0's and 1's.  We can extract just that data with `X = digits.data[(digits.target == 0) | (digits.target == 1)]` and `Y = digits.target[(digits.target == 0) | (digits.target == 1)]`.  Split the data into 80% training and 20% testing.  Create a Perceptron with a learning rate of 1.0 and train it on your training data.  Report the accuracy (i.e. how often the model gives the correct output) on your testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) [1 mark]** Repeat the above, but with the entire data set (i.e. all 10 digits).  Report the accuracy.  How does the accuracy change as you adjust the learning rate?  Make a plot that shows this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) [1 mark]** What mistakes does the model make?  What digits does it tend to mistake for other digits?  Use the `plt.imshow(digits.data[i].reshape(8,8), cmap='gray_r')` command given above to plot some of the digits that it gets wrong.  Why do you think it has problems with these digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "\n",
    "The following code generates the same data that was used to demonstrate curve fitting in class.  `train_x` and `train_y` are the 10 data points we use for doing the curve fitting, and `test_x` and `test_y` are the data we used to test how well the fit generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.RandomState(seed=0)\n",
    "train_x = np.linspace(0, 1, 10)\n",
    "train_y = np.sin(train_x*2*np.pi) + rng.normal(0,0.1,size=10)\n",
    "test_x = np.linspace(0, 1, 500)\n",
    "test_y = np.sin(test_x*2*np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) [1 mark]** Find the weights that best fit this data using linear regression.  This should generate two weights: one that is multiplied by the input value and one that is mulitplied by the feature that is constantly a 1.  Implement this yourself, rather than using the `sklearn.linear_model.LinearRegression` implementation that we will use in Question 4.  To invert the matrix, use `np.linalg.pinv`.  \n",
    "\n",
    "Plot the training data, the ideal testing output, and the actual testing output.  Report the weights found by regression.  Compute and report the Root Mean Squared Error (`np.sqrt(np.mean((Y-output)**2))` where `Y` is the vector of desired outputs and `output` is the vector of the actual outputs from the model) for both the training data and the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) [1 mark]** Repeat part a), but use the first 5 polynomials as features ($x^0, x^1, x^2, x^3, x^4$).  Plot the training data, the ideal testing output, and the actual testing output.  Report the weights found by regression.  Compute and report the Root Mean Squared Error for both the training data and the testing data.  Do not use regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) [1 mark]** Vary the number of polynomials you use from 1 up to 15.  Compute the Root Mean Squared Error for the training and testing data and plot the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) [1 mark]** Now intruduce regularization to your model.  Set the number of polynomials to 10 and vary the amount of regularization.  Use `lambds = np.exp(np.linspace(-50,-1, 50))` to generate the list of 50 different regularization values to try (logarithmically spaces between $e^{-50}$ and $e^{-1}$).  Compute the Root Mean Squared Error for the training and testing data and plot the results.  Note that `plt.semilogx` lets you create a plot where the x-axis is on a log scale, like the version of this plot we saw in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "\n",
    "We will now use the regression tool built in to `sklearn`.  We create it as follows.  Note that it is called `Ridge` due to how regularization is implemented: we add a value onto the diagonal of the matrix being inverted.  You can think of this as adding a diagonal ridge to whatever data is in the matrix.  For this reason, this is often called \"ridge regression\".  The parameter `alpha` sets the amount of regression (it is the same as what we called $\\lambda$ in class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "reg = sklearn.linear_model.Ridge(alpha=0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the regression system using exactly the same functions as the Perceptron.  Here we train it using `X` and `Y`, and then determine what the outputs are given `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X, Y)\n",
    "output = reg.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data, we are going to use the Diabetes dataset from https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html which is also built in to `sklearn`.  You can load this data set using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = sklearn.datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the digits dataset, you can access the `X` values with `diabetes.data` and the `Y` values with `diabetes.target`.  See https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset for an explanation of what the different data values mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) [1 mark]** Split the data evenly into three parts: 1/3rd training, 1/3rd validation, and 1/3rd testing.  This will involve calling `sklearn.model_selection.train_test_split` twice.  Train the model using various different amounts of regularization from $e^{-20)$ to $e^5$ (`lambds = np.exp(np.linspace(-20,5,50))`).  Compute the Root Mean Squared Error on the training and validation datasets and plot how this error changes for different amounts of regularization.  Using these results, pick a good value for regularization and then apply this to your testing data.  Report the Root Mean Squared Error for the testing data.\n",
    "\n",
    "**b) [1 mark]** How consistent is this result?  That is, if you redo part a) but with a different randomly chosen split in the data, do you get the same results?  What overall pattern do you see?  Do the results show signs of overfitting?  Would you expect overfitting here?  Why or why not?\n",
    "\n",
    "**c) [1 mark]** Now let's try regression using polynomials as our features.  Again, `sklearn` has a tool to convert our `X` data into a version with all the polynomials calculated.  Note that our `X` data has 10 inputs ($x_1; x_2; x_3; ... x_{10}$) so when converted to polynomials up to degree 2 it will include $x_1^2, x_1x_2, x_1x_3, ... x_2^2, x_2x_3$, and so on.  Here is how you convert the raw input data into the features `F` that you can then use instead of `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = sklearn.preprocessing.PolynomialFeatures(degree=2).fit_transform(diabetes.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat part a) using the new features.  How does this change the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) [1 mark]** Increase the degree of polynomials used.  Try values up to at least 5.  Compute the same plots as in part a).  How does this change the plots?  Why does this happen?  What happens if you increase the degree up to even larger values like 10 or 20?  Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
